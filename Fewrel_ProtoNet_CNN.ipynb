{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python train_demo.py\n",
    "    train: 10-way 5-shot\n",
    "    val: 5-way 5-shot\n",
    "    batchsize=4,max_length=128\n",
    "    model: ProtoNet, L2 distance\n",
    "    encoder: CNN, hidden_size=230\n",
    "    lr=0.1, SGD, accumulate gradient every iteration,\n",
    "    no dropout, no NoneOfTheAbove\n",
    "86.17%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='./data'\n",
    "name='train_wiki'\n",
    "path = os.path.join(root, name + \".json\")\n",
    "json_data = json.load(open(path))\n",
    "classes = list(json_data.keys()) #64 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='./data'\n",
    "val_dataset='val_wiki'\n",
    "path = os.path.join(root, val_dataset + \".json\")\n",
    "json_data = json.load(open(path))\n",
    "classes = list(json_data.keys()) #16 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P460\n",
      "class length: 700\n",
      "tokens\n",
      "\t Santa Luzia ( Portuguese for Saint Lucia ) is a civil parish in the municipality of São Roque do Pico in the Azores . \n",
      "h\n",
      "\t santa luzia Q2147595 [[0, 1]] \n",
      "t\n",
      "\t saint lucia Q398922 [[5, 6]] \n",
      "Santa\n"
     ]
    }
   ],
   "source": [
    "classid=50\n",
    "print(classes[classid])\n",
    "classdata=json_data[classes[classid]]\n",
    "print('class length:',len(classdata)) # 700 instances\n",
    "instanceid=2\n",
    "for key in classdata[instanceid]:\n",
    "    print(key)\n",
    "    print('\\t',end=' ')\n",
    "    for name in classdata[instanceid][key]:\n",
    "        print(name,end=' ')\n",
    "    print()\n",
    "print(classdata[instanceid]['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Santa', 'Luzia', '(', 'Portuguese', 'for', 'Saint', 'Lucia', ')', 'is', 'a', 'civil', 'parish', 'in', 'the', 'municipality', 'of', 'São', 'Roque', 'do', 'Pico', 'in', 'the', 'Azores', '.']\n",
      "[0, 1] [5, 6]\n"
     ]
    }
   ],
   "source": [
    "classid=50\n",
    "class_name=list(json_data.keys())[classid]\n",
    "j=2\n",
    "item=json_data[class_name][j]\n",
    "print(item['tokens'])\n",
    "print(item['h'][2][0],item['t'][2][0])\n",
    "raw_tokens, pos_head, pos_tail=item['tokens'],item['h'][2][0],item['t'][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=128\n",
    "indexed_tokens = [] # index in dictionary for each tokens in the sentence\n",
    "\n",
    "word2id = json.load(open('./pretrain/glove/glove_word2id.json')) # dict 40w词语包括标点符号、停用词、 [UNK], [PAD]\n",
    "for token in raw_tokens:\n",
    "    token = token.lower()\n",
    "    if token in word2id:\n",
    "        indexed_tokens.append(word2id[token])\n",
    "    else:\n",
    "        indexed_tokens.append(word2id['[UNK]'])\n",
    "\n",
    "# padding\n",
    "while len(indexed_tokens) < max_length: # padding words to max_length\n",
    "    indexed_tokens.append(word2id['[PAD]'])\n",
    "indexed_tokens = indexed_tokens[:max_length]\n",
    "\n",
    "# pos\n",
    "pos1 = np.zeros((max_length), dtype=np.int32)\n",
    "pos2 = np.zeros((max_length), dtype=np.int32)\n",
    "pos1_in_index = min(max_length, pos_head[0]) # the index of the first token of head entity\n",
    "pos2_in_index = min(max_length, pos_tail[0])# the index of the first token of tail entity\n",
    "for i in range(max_length):\n",
    "    pos1[i] = i - pos1_in_index + max_length\n",
    "    pos2[i] = i - pos2_in_index + max_length\n",
    "\n",
    "# mask\n",
    "mask = np.zeros((max_length), dtype=np.int32)\n",
    "mask[:len(indexed_tokens)] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 [2669, 156976, 23, 3578, 10, 2759, 14418, 24, 14, 7, 786, 3363, 6, 0, 2475, 3, 11531, 20228, 88, 31710, 6, 0, 26106, 2, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001, 400001]\n",
      "128 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127]\n",
      "128 [ -5  -4  -3  -2  -1   0   1   2   3   4   5   6   7   8   9  10  11  12\n",
      "  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30\n",
      "  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48\n",
      "  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66\n",
      "  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84\n",
      "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
      " 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120\n",
      " 121 122]\n",
      "128 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "word, pos1, pos2, mask =indexed_tokens, pos1, pos2, mask\n",
    "print(len(word),word)\n",
    "print(len(pos1),pos1-128)\n",
    "print(len(pos2),pos2-128)\n",
    "print(len(mask),mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, K, Q=5,5,5 # N-way K-supportShot Q-queryShot\n",
    "na_rate=0\n",
    "\n",
    "support_set = {'word': [], 'pos1': [], 'pos2': [], 'mask': [] }\n",
    "query_set = {'word': [], 'pos1': [], 'pos2': [], 'mask': [] }\n",
    "query_label = []\n",
    "\n",
    "target_classes = random.sample(classes,N) # N classes from 64\n",
    "for i, class_name in enumerate(target_classes):\n",
    "    indices = np.random.choice(  list(range(len(self.json_data[class_name]))),   K +  Q, False) # K+Q instances from 700 \n",
    "    count = 0\n",
    "    for j in indices:\n",
    "        word, pos1, pos2, mask = self.__getraw__(\n",
    "                self.json_data[class_name][j])\n",
    "        word = torch.tensor(word).long()\n",
    "        pos1 = torch.tensor(pos1).long()\n",
    "        pos2 = torch.tensor(pos2).long()\n",
    "        mask = torch.tensor(mask).long()\n",
    "        if count < self.K:\n",
    "            self.__additem__(support_set, word, pos1, pos2, mask)\n",
    "        else:\n",
    "            self.__additem__(query_set, word, pos1, pos2, mask)\n",
    "        count += 1\n",
    "\n",
    "    query_label += [i] * self.Q\n",
    "return support_set, query_set, query_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model - sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standford GloVe word embedding, 400K+2 vocab，50 embedding\n",
    "# 包括标点符号、停用词，最后两个为[UNK]、[PAD]\n",
    "# 最后一个vocab为[PAD]，pad的embeddding是全部为0的向量\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "glove_mat = np.load('./pretrain/glove/glove_mat.npy') #  numpy.ndarray (400002, 50)\n",
    "glove_word2id = json.load(open('./pretrain/glove/glove_word2id.json')) # dict 40w词语包括标点符号、停用词、 [UNK], [PAD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "class Embedding(nn.Module): #词语及position的embedding\n",
    "\n",
    "    def __init__(self, word_vec_mat, max_length, word_embedding_dim=50, pos_embedding_dim=5):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.max_length = max_length  # 128\n",
    "        self.word_embedding_dim = word_embedding_dim #50\n",
    "        self.pos_embedding_dim = pos_embedding_dim #5\n",
    "        \n",
    "        # Word embedding\n",
    "        # unk = torch.randn(1, word_embedding_dim) / math.sqrt(word_embedding_dim)\n",
    "        # blk = torch.zeros(1, word_embedding_dim)\n",
    "        word_vec_mat = torch.from_numpy(word_vec_mat) #\n",
    "        self.word_embedding = nn.Embedding(word_vec_mat.shape[0], self.word_embedding_dim, padding_idx=word_vec_mat.shape[0] - 1)\n",
    "        self.word_embedding.weight.data.copy_(word_vec_mat) #torch.nn.embedding的weight即为词典各词语的编码\n",
    "\n",
    "        # Position Embedding\n",
    "        self.pos1_embedding = nn.Embedding(2 * max_length, pos_embedding_dim, padding_idx=0)# 256个position 编码维度为5维 第0个词为pad\n",
    "        self.pos2_embedding = nn.Embedding(2 * max_length, pos_embedding_dim, padding_idx=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        word = inputs['word']\n",
    "        pos1 = inputs['pos1']\n",
    "        pos2 = inputs['pos2']\n",
    "        \n",
    "        x = torch.cat([self.word_embedding(word), \n",
    "                            self.pos1_embedding(pos1), \n",
    "                            self.pos2_embedding(pos2)], 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from torch import optim\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, max_length, word_embedding_dim=50, pos_embedding_dim=5, hidden_size=230):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = word_embedding_dim + pos_embedding_dim * 2\n",
    "        self.conv = nn.Conv1d(self.embedding_dim, self.hidden_size, 3, padding=1)  #1层CNN呀？\n",
    "        self.pool = nn.MaxPool1d(max_length)\n",
    "\n",
    "        # For PCNN\n",
    "        self.mask_embedding = nn.Embedding(4, 3) ###########？？\n",
    "        self.mask_embedding.weight.data.copy_(torch.FloatTensor([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0]]))\n",
    "        self.mask_embedding.weight.requires_grad = False\n",
    "        self._minus = -100\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.cnn(inputs)\n",
    "\n",
    "    def cnn(self, inputs):\n",
    "        x = self.conv(inputs.transpose(1, 2))\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x.squeeze(2) # n x hidden_size\n",
    "\n",
    "    def pcnn(self, inputs, mask):\n",
    "        x = self.conv(inputs.transpose(1, 2)) # n x hidden x length\n",
    "        mask = 1 - self.mask_embedding(mask).transpose(1, 2) # n x 3 x length\n",
    "        pool1 = self.pool(F.relu(x + self._minus * mask[:, 0:1, :]))\n",
    "        pool2 = self.pool(F.relu(x + self._minus * mask[:, 1:2, :]))\n",
    "        pool3 = self.pool(F.relu(x + self._minus * mask[:, 2:3, :]))\n",
    "        x = torch.cat([pool1, pool2, pool3], 1)\n",
    "        x = x.squeeze(2) # n x (hidden_size * 3) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class CNNSentenceEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, word_vec_mat, word2id, max_length, word_embedding_dim=50, \n",
    "            pos_embedding_dim=5, hidden_size=230):\n",
    "        nn.Module.__init__(self)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding = Embedding(word_vec_mat, max_length, word_embedding_dim, pos_embedding_dim)\n",
    "        self.encoder = Encoder(max_length, word_embedding_dim, pos_embedding_dim, hidden_size) \n",
    "        self.word2id = word2id\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)  #[200, 128, 60]\n",
    "        x = self.encoder(x)  #[200, 230]\n",
    "        return x\n",
    "\n",
    "    def tokenize(self, raw_tokens, pos_head, pos_tail): # sentence tokens, head entity tokens index, tail entity tokens index\n",
    "        # token -> index\n",
    "        indexed_tokens = [] # index in dictionary for each tokens in the sentence\n",
    "        for token in raw_tokens:\n",
    "            token = token.lower()\n",
    "            if token in self.word2id:\n",
    "                indexed_tokens.append(self.word2id[token])\n",
    "            else:\n",
    "                indexed_tokens.append(self.word2id['[UNK]'])\n",
    "        \n",
    "        # padding\n",
    "        while len(indexed_tokens) < self.max_length: # padding words to max_length\n",
    "            indexed_tokens.append(self.word2id['[PAD]'])\n",
    "        indexed_tokens = indexed_tokens[:self.max_length]\n",
    "\n",
    "        # pos\n",
    "        pos1 = np.zeros((self.max_length), dtype=np.int32)\n",
    "        pos2 = np.zeros((self.max_length), dtype=np.int32)\n",
    "        pos1_in_index = min(self.max_length, pos_head[0]) # the index of the first token of head entity\n",
    "        pos2_in_index = min(self.max_length, pos_tail[0])# the index of the first token of tail entity\n",
    "        for i in range(self.max_length):\n",
    "            pos1[i] = i - pos1_in_index + self.max_length\n",
    "            pos2[i] = i - pos2_in_index + self.max_length\n",
    "\n",
    "        # mask\n",
    "        mask = np.zeros((self.max_length), dtype=np.int32)\n",
    "        mask[:len(indexed_tokens)] = 1\n",
    "\n",
    "        return indexed_tokens, pos1, pos2, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = CNNSentenceEncoder(glove_mat, glove_word2id,  max_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fewshot_re_kit.data_loader import get_loader\n",
    "trainname='train_wiki'\n",
    "trainN,K,Q,na_rate,batch_size=10,5,6,0,4\n",
    "\n",
    "train_data_loader = get_loader(trainname, sentence_encoder,\n",
    "                N=trainN, K=K, Q=Q, na_rate=na_rate, batch_size=batch_size,num_workers=0)\n",
    "support, query, label = next(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 128]) torch.Size([200, 128]) torch.Size([200, 128])\n",
      "torch.Size([240, 128])\n",
      "torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(support['word'].shape,support['pos1'].shape,support['pos2'].shape)\n",
    "print(query['word'].shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset='val_wiki'\n",
    "evalN=5  \n",
    "val_data_loader = get_loader(val_dataset, sentence_encoder,\n",
    "                N=evalN, K=K, Q=Q, na_rate= na_rate, batch_size=batch_size,num_workers=0)\n",
    "support, query, label = next(val_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 128]) torch.Size([100, 128]) torch.Size([100, 128])\n",
      "torch.Size([120, 128])\n",
      "torch.Size([120])\n"
     ]
    }
   ],
   "source": [
    "print(support['word'].shape,support['pos1'].shape,support['pos2'].shape)\n",
    "print(query['word'].shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model script - embedding, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "max_length=128\n",
    "word_vec_mat = torch.from_numpy(glove_mat)\n",
    "word_embedding_dim=50\n",
    "pos_embedding_dim=5\n",
    "word,pos1,pos2 = support['word'],support['pos1'],support['pos2']\n",
    "\n",
    "word_embedding = nn.Embedding(word_vec_mat.shape[0], word_embedding_dim, padding_idx=word_vec_mat.shape[0] - 1) \n",
    "#word_vec_mat.shape[0], 50,400001\n",
    "word_embedding.weight.data.copy_(word_vec_mat) #torch.nn.embedding的weight即为词典各词语的编码      \n",
    "pos1_embedding = nn.Embedding(2 * max_length, pos_embedding_dim, padding_idx=0)# 256个position 编码维度为5维 第0个词为pad\n",
    "pos2_embedding = nn.Embedding(2 * max_length, pos_embedding_dim, padding_idx=0)\n",
    "\n",
    "worde=word_embedding(word)\n",
    "pos1e=pos1_embedding(pos1)\n",
    "pos2e=pos2_embedding(pos2)\n",
    "x=torch.cat([worde,pos1e,pos2e], 2) #[200, 128, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 128]) torch.Size([200, 128]) torch.Size([200, 128])\n",
      "torch.Size([200, 128, 50]) torch.Size([200, 128, 5]) torch.Size([200, 128, 5])\n",
      "torch.Size([200, 128, 60])\n"
     ]
    }
   ],
   "source": [
    "print(word.shape,pos1.shape,pos2.shape)\n",
    "print(worde.shape,pos1e.shape,pos2e.shape)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "hidden_size=230\n",
    "\n",
    "embedding_dim = word_embedding_dim + pos_embedding_dim * 2\n",
    "conv = nn.Conv1d(embedding_dim, hidden_size, 3, padding=1)  #1层CNN呀？\n",
    "pool = nn.MaxPool1d(max_length)\n",
    "\n",
    "x1 = conv(x.transpose(1, 2))\n",
    "x2 = F.relu(x1)\n",
    "x3 = pool(x2)\n",
    "support_emb=x3.squeeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 230, 128])\n",
      "torch.Size([200, 230, 128])\n",
      "torch.Size([200, 230, 1])\n",
      "torch.Size([200, 230])\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)\n",
    "print(x2.shape)\n",
    "print(x3.shape)\n",
    "print(support_emb.shape)  #[200, 230]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model script -- proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hidden_size=230\n",
    "\n",
    "drop = nn.Dropout()\n",
    "\n",
    "support_emb = sentence_encoder(support) # #[200,230] (Batch_size * N * K, hidden_size) \n",
    "query_emb = sentence_encoder(query) #[240, 230] [N*Q*batchsize,hidden_size]\n",
    "support_dr = drop(support_emb)\n",
    "query_dr = drop(query_emb)  #[240, 230] \n",
    "\n",
    "support_vi = support_dr.view(-1, trainN, K, hidden_size) #[4, 10, 5, 230] (Batch_size, N, K, hidden_size)\n",
    "support_me = torch.mean(support_vi, 2) # [4, 10, 230], [batch_size,N,hidden_size] 关于K-shot平均后各类的原型 Calculate prototype for each class\n",
    "x=support_me.unsqueeze(1) #[4, 1, 10, 230]\n",
    "\n",
    "total_Q = Q *trainN + na_rate * Q #60\n",
    "query_vi = query_dr.view(-1, total_Q, hidden_size) # [4, 60, 230]\n",
    "y=query_vi.unsqueeze(2) #[4, 60, 1, 230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 230]) torch.Size([240, 230])\n",
      "torch.Size([4, 10, 5, 230]) torch.Size([4, 10, 230]) torch.Size([4, 1, 10, 230])\n",
      "torch.Size([4, 60, 230]) torch.Size([4, 60, 1, 230])\n"
     ]
    }
   ],
   "source": [
    "print(support_emb.shape,query_emb.shape)\n",
    "print(support_vi.shape, support_me.shape,x.shape)\n",
    "print(query_vi.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=3\n",
    "# dot_xy=x * y #[4, 60, 10, 230] [batchsize, N*Q, N, 230]\n",
    "# logits=dot_xy.sum(dim) #[4, 60, 10] [batchsize, N*Q, N] 各批任务各类各query样本属于各N个类的评分\n",
    "l2_xy=-(torch.pow(x - y, 2))\n",
    "logits=l2_xy.sum(dim) #[4, 60, 10] [batchsize, N*Q, N] 各批任务各类各query样本属于各N个类的评分\n",
    "\n",
    "\n",
    "minn, _ = logits.min(-1) #[4, 60]\n",
    "nona_logits=minn.unsqueeze(2) -1 #[4, 60, 1]\n",
    "logits_with_nona = torch.cat([logits,nona_logits ], 2) # [4, 60, 11] [batchsize, N*Q, N+1]\n",
    "xx=logits_with_nona.view(-1, trainN+1) #[240, 11] [batchsize*N*Q,N+1]\n",
    "_, pred = torch.max(xx, 1) #[240]  batchsize*N*Q个样本的分类预测标签   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 60, 10, 230]) torch.Size([4, 60, 10])\n",
      "torch.Size([4, 60]) torch.Size([4, 60, 1]) torch.Size([4, 60, 11])\n",
      "torch.Size([240, 11]) torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(dot_xy.shape,logits.shape)\n",
    "print(minn.shape,nona_logits.shape,logits_with_nona.shape)\n",
    "print(xx.shape,pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 60, 10, 230]) torch.Size([4, 60, 10])\n",
      "torch.Size([4, 60]) torch.Size([4, 60, 1]) torch.Size([4, 60, 11])\n",
      "torch.Size([240, 11]) torch.Size([240])\n"
     ]
    }
   ],
   "source": [
    "print(l2_xy.shape,logits.shape)\n",
    "print(minn.shape,nona_logits.shape,logits_with_nona.shape)\n",
    "print(xx.shape,pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import autograd, optim, nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Proto(nn.Module):\n",
    "    \n",
    "    def __init__(self, sentence_encoder, dot=False):\n",
    "        nn.Module.__init__(self)\n",
    "        self.sentence_encoder = nn.DataParallel(sentence_encoder)\n",
    "        self.cost = nn.CrossEntropyLoss()\n",
    "        # self.fc = nn.Linear(hidden_size, hidden_size)\n",
    "        self.drop = nn.Dropout()\n",
    "        self.dot = dot\n",
    "        \n",
    "    def loss(self, logits, label):\n",
    "        N = logits.size(-1)\n",
    "        return self.cost(logits.view(-1, N), label.view(-1))\n",
    "\n",
    "    def accuracy(self, pred, label):\n",
    "        return torch.mean((pred.view(-1) == label.view(-1)).type(torch.FloatTensor))\n",
    "\n",
    "    def __dist__(self, x, y, dim):\n",
    "        if self.dot:\n",
    "            return (x * y).sum(dim)\n",
    "        else:\n",
    "            return -(torch.pow(x - y, 2)).sum(dim)\n",
    "\n",
    "    def __batch_dist__(self, S, Q):\n",
    "        return self.__dist__(S.unsqueeze(1), Q.unsqueeze(2), 3)\n",
    "    \n",
    "\n",
    "    def forward(self, support, query, N, K, total_Q):\n",
    "        support_emb = self.sentence_encoder(support) # #[200,230] (B * N * K, D), where D is the hidden size\n",
    "        query_emb = self.sentence_encoder(query) # (B * total_Q, D)\n",
    "        hidden_size = support_emb.size(-1)\n",
    "        support = self.drop(support_emb)\n",
    "        query = self.drop(query_emb)\n",
    "        support = support.view(-1, N, K, hidden_size) # (B, N, K, D)\n",
    "        query = query.view(-1, total_Q, hidden_size) # (B, total_Q, D)\n",
    "        B = support.size(0) # Batch size\n",
    "        support = torch.mean(support, 2) # Calculate prototype for each class\n",
    "        logits = self.__batch_dist__(support, query) # (B, total_Q, N)\n",
    "        minn, _ = logits.min(-1)\n",
    "        logits = torch.cat([logits, minn.unsqueeze(2) - 1], 2) # (B, total_Q, N + 1)\n",
    "        _, pred = torch.max(logits.view(-1, N+1), 1)\n",
    "        return logits, pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Proto(sentence_encoder, dot=True)\n",
    "logits, pred  = model(support, query,  trainN, K, Q *trainN + na_rate * Q)\n",
    "right = model.accuracy(pred, label)\n",
    "grad_iter=1\n",
    "loss = model.loss(logits, label) / float(grad_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 60, 11])\n",
      "torch.Size([240])\n",
      "tensor(0.1083)\n",
      "tensor(6.2720, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape)\n",
    "print(pred.shape)\n",
    "print(right)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run  - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'proto'\n",
    "encoder_name = 'cnn'\n",
    "trainset_name='train_wiki'\n",
    "valset_name='val_wiki'\n",
    "evalN=5 # trainN=10\n",
    "prefix = '-'.join([model_name, encoder_name, trainset_name, valset_name, str(evalN), str(K)])\n",
    "prefix += '-dot'\n",
    "\n",
    "save_ckpt=  'checkpoint/{}.pth.tar'.format(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=1e-1\n",
    "weight_decay=1e-5\n",
    "lr_step_size=20000\n",
    "optimizer = optim.SGD(model.parameters(), learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Proto(\n",
       "  (sentence_encoder): DataParallel(\n",
       "    (module): CNNSentenceEncoder(\n",
       "      (embedding): Embedding(\n",
       "        (word_embedding): Embedding(400002, 50, padding_idx=400001)\n",
       "        (pos1_embedding): Embedding(256, 5, padding_idx=0)\n",
       "        (pos2_embedding): Embedding(256, 5, padding_idx=0)\n",
       "      )\n",
       "      (encoder): Encoder(\n",
       "        (conv): Conv1d(60, 230, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (pool): MaxPool1d(kernel_size=128, stride=128, padding=0, dilation=1, ceil_mode=False)\n",
       "        (mask_embedding): Embedding(4, 3)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cost): CrossEntropyLoss()\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:   30 | loss: 2.332235, accuracy: 13.75%\n",
      "####################\n",
      "\n",
      "Finish training proto-cnn-train_wiki-val_wiki-5-5-dot\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "train_iter=30#000\n",
    "grad_iter=1\n",
    "val_step=2#000\n",
    "start_iter = 0\n",
    "\n",
    "best_acc = 0\n",
    "not_best_count = 0 # Stop training after several epochs without improvement.\n",
    "iter_loss = 0.0\n",
    "iter_loss_dis = 0.0\n",
    "iter_right = 0.0\n",
    "iter_right_dis = 0.0\n",
    "iter_sample = 0.0\n",
    "for it in range(start_iter, start_iter + train_iter):\n",
    "    support, query, label = next(train_data_loader)  \n",
    "    logits, pred  = model(support, query, trainN, K, Q * trainN + na_rate * Q)\n",
    "    loss = model.loss(logits, label) / float(grad_iter)\n",
    "    right = model.accuracy(pred, label)\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "    if it % grad_iter == 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    iter_loss += loss.data.item()\n",
    "    iter_right +=  right.data.item()\n",
    "    iter_sample += 1\n",
    "    sys.stdout.write('step: {0:4} | loss: {1:2.6f}, accuracy: {2:3.2f}%'.format(it + 1, iter_loss / iter_sample, 100 * iter_right / iter_sample) +'\\r')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    if (it + 1) % val_step == 0: #2#000\n",
    "#         acc = self.eval(model, Batch_size, N_for_eval, K, Q, val_iter, na_rate=na_rate, pair=pair)\n",
    "#         model.train()\n",
    "#         if acc > best_acc:\n",
    "#              print('Best checkpoint')\n",
    "#             torch.save({'state_dict': model.state_dict()}, save_ckpt)\n",
    "#             best_acc = acc\n",
    "        iter_loss = 0.\n",
    "        iter_loss_dis = 0.\n",
    "        iter_right = 0.\n",
    "        iter_right_dis = 0.\n",
    "        iter_sample = 0.\n",
    "\n",
    "print(\"\\n####################\\n\")\n",
    "print(\"Finish training \" + prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Proto(\n",
       "  (sentence_encoder): DataParallel(\n",
       "    (module): CNNSentenceEncoder(\n",
       "      (embedding): Embedding(\n",
       "        (word_embedding): Embedding(400002, 50, padding_idx=400001)\n",
       "        (pos1_embedding): Embedding(256, 5, padding_idx=0)\n",
       "        (pos2_embedding): Embedding(256, 5, padding_idx=0)\n",
       "      )\n",
       "      (encoder): Encoder(\n",
       "        (conv): Conv1d(60, 230, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (pool): MaxPool1d(kernel_size=128, stride=128, padding=0, dilation=1, ceil_mode=False)\n",
       "        (mask_embedding): Embedding(4, 3)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cost): CrossEntropyLoss()\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] step: 1000 | accuracy: 29.33%\n"
     ]
    }
   ],
   "source": [
    "val_iter=1000      \n",
    "\n",
    "iter_right = 0.0\n",
    "iter_sample = 0.0\n",
    "with torch.no_grad():\n",
    "    for it in range(val_iter):\n",
    "        support, query, label = next(val_data_loader)\n",
    "        logits, pred = model(support, query, evalN, K, Q * evalN + Q * na_rate)\n",
    "        right = model.accuracy(pred, label)\n",
    "        iter_right += right.data.item()\n",
    "        iter_sample += 1\n",
    "        sys.stdout.write('[EVAL] step: {0:4} | accuracy: {1:3.2f}%'.format(it + 1, 100 * iter_right / iter_sample) +'\\r')\n",
    "        sys.stdout.flush()\n",
    "    print(\"\")\n",
    "acc=iter_right / iter_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
